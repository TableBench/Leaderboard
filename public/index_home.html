<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8" />
  <meta name="description" content="TableBench: A Comprehensive and Complex Benchmark for Table Question Answering" />
  <meta name="keywords"
    content="Table Question Answering, Table QA, Large Language Models, LLM, Table QA Evaluation, Benchmark" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>
    TableBench Homepage
  </title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="./css/bulma.min.css" />
  <link rel="stylesheet" href="./css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="./css/bulma-slider.min.css" />
  <link rel="stylesheet" href="./css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="./css/index.css" />
  <link rel="icon" href="./images/favicon.svg" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./js/fontawesome.all.min.js"></script>
  <script src="./js/bulma-carousel.min.js"></script>
  <script src="./js/bulma-slider.min.js"></script>
  <script src="./js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              TableBench: A Comprehensive and Complex Benchmark for Table Question Answering
            </h1>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="">Xianjie Wu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Jian Yang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Linzheng Chai</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Ge Zhang</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="">Jiaheng Liu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Xeron Du</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="">Di Liang</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="">Daixin Shu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Xianfu Cheng</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Tianzhen Sun</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Tongliang Li</a><sup>4</sup>,</span>
              <span class="author-block">
                <a href="">Zhoujun Li</a><sup>1</sup>,</span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>CCSE, Beihang University</span>
              <span class="author-block"><sup>2</sup>University of Waterloo</span>
              <span class="author-block"><sup>3</sup>Fudan University</span>
              <span class="author-block"><sup>4</sup>Beijing Information Science and Technology University</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/TableBench/TableBench"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- TableBench Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/Multilingual-Multimodal-NLP/TableBench"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>TableBench</span>
                  </a>
                </span>
                <!-- TableInstruct Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/Multilingual-Multimodal-NLP/TableBench-Instruct"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>TableInstruct</span>
                  </a>
                  <!-- TableLLM Link. -->
                  <span class="link-block">
                    <a href="https://huggingface.co/collections/Multilingual-Multimodal-NLP/tablebench-6698d0b29a21ed71c819f30e"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="far fa-images"></i>
                      </span>
                      <span>TableLLMs</span>
                    </a>
                  </span>
                  <!-- Leaderboard Link. -->
                  <span class="link-block">
                    <a href="leaderboard.html" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-trophy"></i>
                      </span>
                      <span>Leaderboard</span>
                    </a>
                  </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="columns is-centered">
          <!-- center the image -->
          <img src="./images/intro_case.png" alt="Teaser" class="teaser-image center" width="60%" />
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <p>
              Recent advancements in Large Language Models (LLMs) have markedly enhanced the processing and interpretation of tabular data, introducing previously unimaginable capabilities. 
              Despite these achievements, LLMs still encounter significant challenges when applied in industrial scenarios, particularly due to the increased complexity of reasoning required with real-world tabular data, underscoring a notable disparity between academic benchmarks and practical applications. 
              To address this discrepancy, we conduct a detailed investigation into the application of tabular data in industrial scenarios and propose a Comprehensive and Complex Table Benchmark (TableBench), including 18 fields within 4 major categories of table question-answering capabilities.
              Additionally, we introduce TableLLM, trained on TableInstruct, specifically designed to enhance reasoning efficiency in table question-answering tasks. 
              Massive experiments conducted on TableBench indicate that both open-source and proprietary LLMs still have significant room for improvement to meet real-world demands, where the most advanced model, GPT-4, achieves only a modest score compared to human performance.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Dataset Construction</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/annotation_process.png" alt="Dataset Construction" class="teaser-image center"
                width="80%" height="80%" />
            </div>
            <p>
              We introduce TableBench for evaluation, consisting of 886 instances in total, which includes tables,
              questions, and their corresponding answers, covering 18 typical scenarios for table-based question
              answering.
              Additionally, we construct TableInstruct aimed at instruct-tuning, incorporating tables, questions,
              answers, and detailed reasoning information.
              The entire dataset construction process is shown in Figure.
            </p>
          </div>
        </div>
      </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Dataset Statistic</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <div class="image-container">
                <img src="./images/overview_of_tablebench.png" alt="Overview of TableBench" class="teaser-image left" />
                <div class="right-images">
                  <img src="./images/reasoning_steps_analysis.png" alt="Reasoning Steps Analysis"
                    class="teaser-image" />
                  <img src="./images/categories.png" alt="Dataset Topics" class="teaser-image" />

                </div>
              </div>
            </div>
            <h4>
              Overview
            </h4>
            <p>
              Drawing from real-world scenarios and user demands for tabular data, we devise four primary question
              categories: fact-checking, numerical reasoning, data analysis, and visualization, thoroughly illustrating
              the various challenges encountered in table question-answering scenarios.
              Relative to existing datasets, TableBench encompasses a broader spectrum of question categories, with a
              particular emphasis on data analysis and chart plotting capabilities that are notably scarce in prior
              datasets.
            </p>
            <h4>
              Complexity Analysis
            </h4>
            <p>
              We define the complexity of the dataset by calculating the number of reasoning steps required to solve the
              problem. Figure above demonstrates that the overall complexity of TableBench is
              significantly higher than that of existing datasets. 
            </p>
          </div>
        </div>
      </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overall Performance</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/overall_results.png" alt="Dataset Statistic" class="teaser-image center" width="100%"
                height="80%" />
            </div>
            <p>
              We evaluate 30+ models with sizes ranging from 7B to 110B parameters, including general/code LLMs, open-source/proprietary models, and SFT~\citep{ouyang2022training} models.
              We explore three distinct reasoning methodologies to augment the reasoning capabilities for tabular data: Textual Chain of Thought (TCoT), Symbolic Chain of Thought (SCoT), and Program of Thought (PoT).
              TCoT utilizes a textual reasoning approach, employing a series of inferential steps to deduce the final answer.
              SCoT adopts symbolic reasoning steps, leveraging programming language commands to iteratively simulate and refine results through a 'Think then Code' process. 
              Conversely, PoT involves generating executable code, employing lines of code as reasoning steps within a programming environment to achieve results.
            </p>
          </div>
        </div>
      </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Further Analysis</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/effect_of_parsing_ratio.png" alt="Dataset Statistic" class="teaser-image center" width="50%"
                height="80%" />
              <img src="./images/data_efficient.png" alt="Dataset Statistic" class="teaser-image center" width="50%"
                height="80%" />
            </div>
            <h4>
              Effect of Parsing Ratio
            </h4>
            <p>
              The performance curve associated with the PoT strategy indicates a marked improvement in overall scores as parsing ratios increase, highlighting the significant potential of the PoT method to boost overall scores through enhanced code pass rates. Comparing the DP, SCoT, and TCoT strategies, data points on the left side of the quadratic curve represent anomalies where performance exceeds expectations at low parsing ratios, where some models (e.g. StructLLM) have weak instruction-following capability compared to the strong capabilities of the table understanding. The right parts of the curves, such as TCoT, show more significant potential at higher resolutions.
            </p>
            <h4>
              Data Efficiency of TableInstruct
            </h4>
            <p>
              In this section, we explore the effects of random subsampling on the TableInstruct mixture for TableLLM, with sampling rates ranging from 0.2 to 0.6, to enhance data efficiency further. Figure above intuitively displays the relative performance of different sampling variants. Initially, the full data mixture provides the highest knowledge coverage, enabling the model to achieve optimal overall performance. Surprisingly, with only 60\% of the samples, the model still retains over 90\% of the full dataset's performance. We observe that the Llama-3-8B model requires fewer than 4000 samples to surpass the performance of Qwen1.5-70B on the dataset, significantly enhancing the performance of smaller-scale models. When fine-tuning across the entire training dataset for each task, Llama-3.1-8B demonstrates comparable performance to GPT-3.5, with the inference cost being only a fraction of that of GPT-3.5, indicating potential for further improvement in data efficiency.
            </p>  


          </div>
        </div>
      </div>
  </section>


  <footer class="footer">
    <div class="container">

      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              Please reach out to <a href="wuxianjie@buaa.edu.cn">wuxianjie@buaa.edu.cn</a> for questions or
              feedback on TableBench. We are also open to collaborations and suggestions for new scenarios to add to
              the benchmark.
            </p>
            <p>
              The source code from this website is borrowed from <a
                href="https://github.com/LiveCodeBench/livecodebench.github.io">LiveCodeBench</a>!
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>