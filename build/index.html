<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8" />
  <meta name="description" content="TableBench: A Comprehensive and Complex Benchmark for Table Question Answering" />
  <meta name="keywords" content="Table Question Answering, Table QA, Large Language Models, LLM, Table QA Evaluation, Benchmark" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>
    TableBench Homepage
  </title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="./css/bulma.min.css" />
  <link rel="stylesheet" href="./css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="./css/bulma-slider.min.css" />
  <link rel="stylesheet" href="./css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="./css/index.css" />
  <link rel="icon" href="./images/favicon.svg" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./js/fontawesome.all.min.js"></script>
  <script src="./js/bulma-carousel.min.js"></script>
  <script src="./js/bulma-slider.min.js"></script>
  <script src="./js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              TableBench: A Comprehensive and Complex Benchmark for Table Question Answering
            </h1>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="">Xianjie Wu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Jian Yang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Linzheng Chai</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Ge Zhang</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="">Jiaheng Liu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Xeron Du</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="">Di Liang</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="">Daixin Shu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Xianfu Cheng</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Tianzhen Sun</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Tongliang Li</a><sup>4</sup>,</span>
              <span class="author-block">
                <a href="">Zhoujun Li</a><sup>1</sup>,</span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>CCSE, Beihang University</span>
              <span class="author-block"><sup>2</sup>University of Waterloo</span>
              <span class="author-block"><sup>3</sup>Fudan University</span>
              <span class="author-block"><sup>4</sup>Beijing Information Science and Technology University</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/TableBench/TableBench"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/Multilingual-Multimodal-NLP/TableBench"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>TableBench</span>
                  </a>
                </span>

                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/Multilingual-Multimodal-NLP/TableBench-Instruct"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>TableBench-Instruct</span>
                  </a>
                </span>
                <!-- Leaderboard Link. -->
                <span class="link-block">
                  <a href="leaderboard.html" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-trophy"></i>
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="columns is-centered">
          <!-- center the image -->
          <img src="./images/mceval_intro.png" alt="Teaser" class="teaser-image center" width="60%" />
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <p>
              TableBench is a comprehensive and complex benchmark, covering \textbf{18} fields within four major categories of table question-answering capabilities.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Dataset Statistics</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/1_statistics.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              We plot the length of input length, the length of output, and the number of
              test cases of each programming language. The multilingual code generation and explanation tasks
              separately contain 2K samples, where each language has nearly 50 samples. The code completion
              task can be decomposed into multi-line completion (3K samples), single-line completion (3K samples),
              span completion (4K samples), and span completion (light) (2K samples).
            </p>
          </div>
        </div>
      </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"> Human Annotation & Quality Control</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/2_bench_cases.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              To create the massively multilingual code evaluation benchmark McEval, the annotation of multilingual
              code samples is conducted utilizing a comprehensive and systematic human annotation procedure,
              underpinned by rigorously defined guidelines to ensure accuracy and consistency.
              Following a detailed training session on the annotation protocol, which emphasizes the importance of
              context,
              syntactical correctness, and semantic fidelity across languages, annotators are tasked with creating
              problem definitions and
              the corresponding solution.
            </p>
          </div>
        </div>
      </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">mCoder</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/3_framework.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>

            </p>
          </div>
        </div>
      </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Further Analysis</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/4_code_class.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              We categorize the programming languages of McEval into 5 programming paradigms and 11 application
              scenarios and summarize the performance of code LLMs on the code generation task.
            </p>
          </div>
        </div>
      </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Model Performance in Code Completion Tasks</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/5_radar_class_result.png" alt="HumanEval Overfitting" class="teaser-image center"
                width="80%" height="80%" />
            </div>
            <p>
              It can be observed that code LLMs generally perform better in object-oriented and multi-paradigm
              programming languages (high-resource languages), while perform worse in functional and procedural
              programming languages (low-resource languages).
              In areas like web development and scientific computing, the gap between open-source and closed-source
              models is narrowing. However, for application scenarios, there is still a substantial gap
              between open-source models and the closed-source GPT-4 series in low-resource languages related to
              scripting, mobile development, and educational research. mCoder performs superior over multiple same-size
              models and even some larger open-source models.
            </p>
          </div>
        </div>
      </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Unbalance in Different Languages</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/6_cross_dataset.png" alt="HumanEval Overfitting" class="teaser-image center"
                width="80%" height="80%" />
            </div>
            <p>
              We compare the results of several open-source models on the Multipl-E multilingual benchmark with
              corresponding languages on McEval.
              We obtained scores for 11 programming languages (including Python, Java, JavaScript, C++, PHP, Rust,
              Swift, R, Lua, Racket, and Julia) from the BigCode
              leaderboard. As shown in Figure (1), due to the simplicity of Python language tasks in this dataset, many
              models exhibit significant score discrepancies between the
              two benchmarks. By examining Figure (2) and (3), it becomes evident that all models demonstrate
              consistent multilingual capabilities between Multipl-E and McEval. However, Figure (2) highlights
              a majority of models within the blue circle, indicating that the current state-of-the-art performance
              of most models primarily lies in high-resource languages like Python, while their proficiency in
              low-resource languages awaits further exploration and enhancement.
            </p>
          </div>
        </div>
      </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Cross-lingual Transfer</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/7_cross-lingual-transfer.png" alt="HumanEval Overfitting" class="teaser-image center"
                width="80%" height="80%" />
            </div>
            <p>
              We fine-tune the CodeQwen-1.5 model using Python-only data in McEval-Instruct
              and compare it with mCoder. CodeQwen-1.5 performs well in most high-resource languages, but CodeQwen
              without alignment exhibits unsatisfactory results in
              some low-resource languages due to the inability to follow instructions. As such, with fine-tuning using
              only Python data,
              CodeQwen-1.5-Python improves significantly across most languages. It shows that the CodeQwen foundation
              model already possesses strong coding capabilities
              but lacks adequate instruction-following skills. Therefore, fine-tuning with Python-only data can still
              effectively transfer instruction-following abilities to other languages,
              resulting in superior multilingual performance.
            </p>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Examples of Multilingual Generation</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/9_bench_completion_cases.png" alt="HumanEval Overfitting" class="teaser-image center"
                width="80%" height="80%" />
            </div>
            <p>
              The data mainly consists of an instruction part
              (including function name, function description, and function call cases),a reference solution, and
              a test cases part. Left Figure: an example of the Lisp language. Middle Figure: a file processing
              programming task in AWK language. During the evaluation, the corresponding file processing result
              by the generated code will be compared with the reference answer. Right Figure: an example of the R
              language.
            </p>
          </div>
        </div>
      </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Examples of Multilingual Explanation</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/10_bench_explain_cases.png" alt="HumanEval Overfitting" class="teaser-image center"
                width="80%" height="80%" />
            </div>
            <p>
              The data mainly consists of an instruction part (including a complete function),
              a reference Explanation. Left Figure: an example of the Kotlin language. Middle Figure: an example of
              the Lua language. Right Figure: an example of the HTML language.
            </p>
          </div>
        </div>
      </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Examples of Multilingual Completion</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/11_bench_fim_cases.png" alt="HumanEval Overfitting" class="teaser-image center"
                width="80%" height="80%" />
            </div>
            <p>
              The data mainly consists of an instruction part
              (including an incomplete function ), a reference complete code solution and test cases. Left Figure: an
              span completion example of the C++ language. Middle Figure: a single-line completion example of
              the Rust language. Right Figure: a multiple-line completion example of the Shell language.
            </p>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Analysis of Language Representations</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/12_cluster_merge.png" alt="HumanEval Overfitting" class="teaser-image center"
                width="80%" height="80%" />
            </div>
            <p>
              As shown in the Figure, we analyzed the programming languages in the McEval from their presentation
              perspective.
              We used CodeBERT to extract code representations from code snippets in McEval.
              The figure clearly shows that languages with similar syntax have closely related representations.
              For example, other functional programming languages similar to CommonLisp, as well as C, C++, Java, and
              scripting
              languages, exhibit high grammar similarity.
            </p>
          </div>
        </div>
      </div>
  </section>

  <!--
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code></code></pre>
    </div>
  </section>
-->
  <footer class="footer">
    <div class="container">

      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              Please reach out to <a href="challenging@buaa.edu.cn">challenging@buaa.edu.cn</a> for questions or
              feedback on McEval. We are also open to collaborations and suggestions for new scenarios to add to
              the benchmark. Finally, McEval provides one axis of LLM coding evaluations and we recommend the
              following leaderboards for measuring code LM ability on various coding tasks, such as
              <a href="https://livecodebench.github.io/leaderboard.html">LiveCodeBench Leaderboard</a>,
              <a href="https://evalplus.github.io/leaderboard.html">EvalPlus Leaderboard</a>,
              <a href="https://crux-eval.github.io/leaderboard.html">CruxEval Leaderboard</a>,
              <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">Chatbot Arena Leaderboard</a>,
              <a href="https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard">BigCode Models Leaderboard</a>,
              <a href="https://infi-coder.github.io/inficoder-eval/">InfiCoder-Eval</a>, and
              <a href="https://leaderboard.tabbyml.com/">TabbyML Leaderboard</a>.
            </p>
            <p>
              The source code from this website is borrowed from <a
                href="https://github.com/LiveCodeBench/livecodebench.github.io">LiveCodeBench</a>!
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>